

%  while true ; do rsync /home/imaseki/OneDrive/C105/C105Fig/* ./C105Fig/color/ ; python3 ./python/convert_gray_scale.py --input-dir ./C105Fig/color --output-dir ./C105Fig/gray  ; sleep 30 ; done

% 画像の配置は最後に再度確認する

% 推敲TODO
% 節タイトルはパワポ感覚でつけていきたい。
% 引用の体裁を整える
% 実験結果の修正

\documentclass[a5paper,twoside,dvipdfmx]{jsarticle}
\AtBeginDvi{\special{pdf:mapfile haranoaji.map}}
\usepackage[dvipdfmx]{graphicx}
\title{実践PEFT

～ご家庭のGPUでLLM fine-tuning～}
%\author{お椀の底の玉}
\date{}

\usepackage{ascmac}

\usepackage{caption}
\captionsetup[table]{font={scriptsize}}

\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}

\usepackage[top=30truemm,bottom=20truemm,left=25truemm,right=25truemm,headsep=5truemm]{geometry}

\usepackage{url}

\usepackage{color}

\usepackage{type1cm}

\usepackage{bxpapersize}

\usepackage{booktabs}

\usepackage{listings,jlisting} %日本語のコメントアウトをする場合jlistingが必要
%ここからソースコードの表示に関する設定
\lstset{
  language=Python,
  basicstyle={\ttfamily},
  identifierstyle={\small},
%  commentstyle={\smallitshape},
  keywordstyle={\small\bfseries},
  ndkeywordstyle={\small},
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},
  numbers=left,
  xrightmargin=0zw,
  xleftmargin=0zw,
  numberstyle={\scriptsize},
  stepnumber=1,
  numbersep=1zw,
  lineskip=-0.5ex,
  showstringspaces=\false
}

\pagestyle{empty}

\usepackage{fancyhdr}  
\pagestyle{fancy}
\fancyhead[RO,LE]{\thepage}
\fancyhead[RE,LO]{\nouppercase{\leftmark}}
\fancyheadoffset[R,L]{1cm}
\cfoot{ }


\usepackage{jumoline}
\setlength{\UnderlineDepth}{3pt}

\begin{document}

% \mcfamily	

\tableofcontents

\newpage

\maketitle

\fontsize{9pt}{16pt}\selectfont

\section{はじめに}

\subsection{本書の目的}

ChatGPTが登場してから2年、LLMは世界を席巻しバズワードの代表格となりました。今やAIによるちょっとした推論なら、プログラムを書かずとも命令の文を送信すれば簡単に実行できます。プロンプトエンジニアリング時代の到来です。

しかし、だからと言って機械学習が不要になったわけではありません。プロンプトエンジニアリングで達成できる性能は限定的であり、教師データを用いて直接モデルの重みパラメータを調整する方がより高い精度を狙うことができます。またプロンプトエンジニアリングはモデルが変わると最適なプロンプトも変わってくるため再度プロンプトを実装する必要がある一方で、fine-tuningであれば（モデル自体の性能差はともかくとして）学習を実行するデータとプログラムは同じものを使いまわすことができます。

一方でLLMは非常に巨大なため、個人のGPUでfine-tuningを行うのは困難です。GPT-3.5のパラメータ数は約3550億で\footnote{なおGPT-4のパラメータ数は非公開です。}、これを全て32bit浮動小数点で扱うとするとモデルをメモリに載せるだけで1.4TBも必要になります。一方で個人で購入できるGPUのメモリは比較的購入しやすいもので8GB～12GB、業務用に片足を突っ込んだ価格のハイエンドモデルでも24GBしかありません。さすがにこれでは学習どころか推論もままならないため、この本でも取り扱いはしません。

そこで一回りパラメータ数が小さいGPT-2を見てみましょう。こちらはパラメータ数15億で、32bitのfloatなら6GBで収まります。ですがこれならfine-tuningもできる……とはいきません。推論時と異なり、学習時にはモデル自体のパラメータ数と同じだけ勾配と呼ばれる次のパラメータ更新時の差分を管理するためのメモリ容量も必要になります。つまり6GB * 2 = 12GBが必要です。fine-tuningの手法によっては勾配に加えてそのモーメントもモデルのサイズ分, またはこの2倍保持しておく必要があり、これも含めると最大で6GB * 4 = 24GBも要求します。さらには学習時にバッチごとに実行する勾配順伝播・逆伝播にもバッチサイズに比例してGPUメモリは必要となり、庶民的なGPUでは動かすのは厳しいです。

またメモリにどうにか載せたとしても、LLMのfine-tuningは精度を出すのが難しいです。事前学習済みなので一から学習するよりはデータが少なくて済むものの、パラメータ数が多い分fine-tuningにも大量のデータが必要になるでしょう。

これらの問題を一挙に解決するのが、本書の主題であるParameter Efficient Fine Tuning (\textsf{PEFT})です。PEFTはLLMのパラメータは固定のままfine-tuningします。どういうことかというと、LLMの入力であったり、何重にも重なったレイヤーの間であったりに追加で少数のパラメータをアダプター的に接続し、これだけを学習するのです。どこにどのようなパラメータを組み込むかは手法によって異なり、これらの総称がPEFTです。

学習するパラメータを最小限にすることで勾配やモーメントに必要なメモリはごくわずかで済むため、モデル自体がメモリに載ればあとはバッチサイズ次第でfine-tuningを動かすことができます。またパラメータが少ないため、データ数が少ない状況下でもきちんと精度を出すことが可能です。

しかし一体どうやって少数のパラメータだけでデータを学習できるのでしょうか？　本書ではそのテクニックを大きく3つのカテゴリに分けて紹介していきたいと思います。

\subsection{本書の内容}

本書はさまざまなPEFT手法を紹介しつつ、テキスト分類タスクに対してその手法で公開LLMをfine-tuningし、ある程度の精度が出ることを通して学習の成功を確認します。せっかくLLMのfine-tuningを行うならテキスト生成のタスクにfine-tuningを行いたいところですが、これだときちんと学習できているかどうかを定量的に評価することが難しいです。そこでAccuracyやAUCといった数値で明確に評価可能なテキスト分類タスクを利用するというわけです。

構成について、まず2章で導入としてタスクやベースとするLLMについて整理し、そのfine-tuningがご家庭のGPUでは素朴には難しいことを示します。その後3～5章で大まかに3種類あるPEFT手法について解説しつつ、実際にポエム分類タスクを解かせてその結果を見ていきます。

本書の実験は実際にpythonのプログラムを書いて実施しており、解説のためその一部を載せることがあります。ただしプログラミング解説書という位置づけではないので、実装のすべてを丁寧に説明することはしていません。あくまでpythonが読める人向けの挿絵のようなものであり、読み飛ばしてしまっても問題ありません。実装の全体は \url{https://github.com/jntlnlnd/C105-llm-peft} にアップロードしていますが、こちらも解説というほどの整理はしてないため、あくまでも参考程度となります。もし手元で実行したい場合はあらかじめ以下のライブラリをインストールしてください。バージョンについては多少違っても動くと思われます。

\begin{lstlisting}
pip install \
  torch==2.5.1 \
  transformers==4.46.3 \
  datasets==3.1.0 \
  evaluate==0.4.3 \
  scikit-learn==1.5.2 \
  peft==0.13.2
\end{lstlisting}

それでは前置きはこれくらいにして、さっそくPEFTを試してみましょう！

\newpage

\section{導入}

\subsection{タスクの準備}

本書で題材に選んだのはBLEACH・COMIC LOのポエム分類タスクです。BLEACHはコミックスの巻頭に、LOは表紙にそれぞれポエムが書いてあるのですが、これを分類していくというものです。例えば以下の2つのポエムについて、どちらの作品かわかるでしょうか？

\begin{itemize}
  \item お前には一生、勝てない気がする。
  \item 伏して生きるな、立ちて死すべし
\end{itemize}

ちょっと難しいですね。もちろん露骨にわかりやすいポエムもあるため、両者に詳しくない人でも8～9割くらいは正解できる難易度感です。

なおこのタスクは既刊の『BLEACH・LOポエム分類でたどる言語処理技術の発展』でも取り扱っています。そちらも合わせてご覧ください。

集めたポエムは学習用のtrainデータ・epoch毎の評価を行うためのvalidデータ\footnote{本書では簡便のためEarlyStoppingやハイパーパラメータたんさくなどは行っていません}・最終的な評価のtestデータに分けました。各区分のデータサイズは以下の通りです。

\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
     & BLEACH & LO \\
    \hline
    train & 42 & 120 \\
    \hline
    valid & 16 & 38 \\
    \hline
    test & 16 & 38  \\
    \hline
  \end{tabular}
\end{table}

具体的な実装についても少し触れます。ポエムのデータはtsvとして保存され、`huggingface/datasets'ライブラリで読み込みます。

tsvは以下のようなものとなります。

\begin{lstlisting}
$ head -5 train.tsv
poem	label
誇りを一つ捨てるたび我等は獣に一歩近付く心を一つ殺すたび我等は獣から一歩遠退く	0
初恋は、歳上でした。	1
一緒に数えてくれるかい君についた僕の歯型を	0
まっ白いお米は、どろんこからできます。	1
"てごわい敵とバトルだ！""マスク少女萌え""とか言ってる場合じゃない!?"	1
\end{lstlisting}

labelが0だとBLEACH, 1だとCOMIC LOから転記したものです。

これを以下のソースコードで読み込みました。

\begin{lstlisting}
from datasets import load_dataset
ds = load_dataset(
  "csv",
  data_files={
      "train": "../data/train.tsv",
      "valid": "../data/valid.tsv",
      "test": "../data/test.tsv",
  },
  delimiter="\t",
).rename_column("label", "labels")
\end{lstlisting}

非常に簡単に読み込めますね。補足しておくと、labelをlabelsにrenameしているのは、この後学習に利用するtransformersというライブラリのお作法だと思ってください。

本データについて1点注意があります。このデータセットは全体でも250行と非常に小さく、本書で紹介するPEFTの各手法を精度比較することはとてもできません。それぞれの手法について学習結果として精度も掲載していますが、あくまできちんと学習が行われたというデバッグを超える意味は持たないことを念頭に置いてください。


\subsection{公開LLMの用意}

本書ではベースとして`llm-jp/llm-jp-3-1.8b'\footnote{\url{https://huggingface.co/llm-jp/llm-jp-3-1.8b}}というLLMを用います。これは国立情報学研究所が開発した日本語特化のLLMで、パラメータ数は1.8B（18億）以上に及びます。3.7Bや13Bなどより大きなサイズのモデルもありましたが、これらは私のGPUのメモリにはとても乗らないので1.8Bを選定しました。

1.8Bと小さめとはいえ、LLMとして決して見劣りしない性能は備えています。試しにテキストを与えて続きを生成させてみましょう。

\begin{lstlisting}
from transformers import pipeline

text_pipe = pipeline('text-generation', model="llm-jp/llm-jp-3-1.8b", device="cuda")

print(text_pipe("BLEACHとは", max_length=100)[0]['generated_text'])
\end{lstlisting}

\begin{figure}[h]
  \centering
  \includegraphics[width=100mm]{../C105Fig/gray/generate.png}
  %\caption{}
  %\label{fig:GoogleColab}
 \end{figure} 

\newpage

文法的にも違和感のない、いい感じの生成結果を得ることができました。目次が挿入されているのはwikipediaか、外部リンクという項目を踏まえるとpixiv大百科あたりの影響を受けている可能性が高いです。事前学習のデータにはCommon CrawlといったWebページのテキストデータが使われているので\footnote{\url{https://llmc.nii.ac.jp/topics/llm-jp-172b/}}、この中にpixiv大百科があってそのテキストを学習したのかもしれません。

WebページのテキストからBLEACHのことを学習済みなら、ポエムも学習済みなのでしょうか？　もし学習済みだとすると、fine-tuningで精度が出せた時の結果に対して、ポエムの微妙な表現の違いを学習しているのではなく事前に記憶したポエムを思い出しているだけではというように考察が変わってきます。

試しにテキストを与えてみましょう。

\begin{lstlisting}
print(text_pipe("""以下はBLEACHの巻頭のキャッチフレーズです。続きを埋めてください: 
伏して生きるな、""", max_length=30)[0]['generated_text'])
\end{lstlisting}

\begin{figure}[h]
  \centering
  \includegraphics[width=100mm]{../C105Fig/gray/generate_2.png}
  %\caption{}
  %\label{fig:GoogleColab}
 \end{figure} 


だいぶ直球なポエムになりましたね。これを以って学習データにポエムのテキストが含まれていないかという判断は難しいですが、少なくともポエムをはっきり記憶しているわけではなさそうです。

上記の通り日本語能力はそこそこある一方で、ポエム自体の丸暗記まではしていない本モデルがきちんとfine-tuningでポエムの表現の傾向を学習できるかが次章以降のポイントとなります。

\subsection{試しに全パラメータのfine-tuningをしてみる}

PEFT手法を試す前に、本モデルを一般的な手法でfine-tuningすることが難しいことを確認しましょう。まずモデルを読み込んだ段階でのGPUの使用率を`nvidia-smi'コマンドで確認した結果が以下の通りです。

\begin{figure}[h]
  \centering
  \includegraphics[width=100mm]{../C105Fig/gray/nvidia-smi.png}
  %\caption{}
  %\label{fig:GoogleColab}
 \end{figure} 

GPUにはRTX2070を使っています。GPUメモリのサイズは8GBで廉価GPUでは一般的ですが、モデルを載せるだけでGPUメモリの大半を使い切っています。ざっくり32bit浮動小数点なので1パラメータあたり4Byte、1.8Bパラメータのモデルなので単純計算で7.2GBほど消費する計算です。本GPUは画面描画にも用いているためそちらにもメモリを割いていることを踏まえると、8GBのほとんどを使い切っているのは想定通りといったところです。

推論だけなら何とか動きますが、学習しようとするとこれでは足りません。実際にfine-tuningのコードを組んでみましょう。

まずはモデルを読み込みます。transformersのライブラリを使うと簡単に実施できます。

\begin{lstlisting}
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
)

tokenizer = AutoTokenizer.from_pretrained("llm-jp/llm-jp-3-1.8b")
model = AutoModelForSequenceClassification.from_pretrained("llm-jp/llm-jp-3-1.8b")
# 後のTrainerで以下を実行しないと失敗するため入れている設定
model.config.pad_token_id = tokenizer.pad_token_id

\end{lstlisting}

続いて前処理用のクラスを定義します。LLMをはじめとする機械学習モデルは数値計算からなるため、テキストデータはそのままでは扱うことができません。そこでテキストを単語（のようなもの）に分割し、それぞれ対応するIDとして数字列にしたうえでモデルに入力します。それを行うのがtokenizerで、それを動かすためのクラスになります。

\begin{lstlisting}
# テキストのtokenizeを行うための前処理クラス
class TokenizeCollator:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, examples):
        encoding = self.tokenizer(
            [ex["poem"] for ex in examples],
            padding="longest",
            truncation=True,
            max_length=200,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"],
            "attention_mask": encoding["attention_mask"],
            "labels": torch.tensor([ex["labels"] for ex in examples]),
        }

\end{lstlisting}

続いてmetricsを計算する関数です。これは学習途中でのvalidデータに対する精度や、学習完了後のtestデータに対する精度を出す関数となっています。

\begin{lstlisting}
import evaluate

roc_auc_evaluate = evaluate.load("roc_auc")
acc_evaluate = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = map(torch.tensor, eval_pred)
    probs = torch.nn.functional.softmax(logits, dim=1)[:, 1]  # label=1の確率
    pred_labels = torch.argmax(logits, dim=1)  # 予測ラベル
    return {
      **roc_auc_evaluate.compute(prediction_scores=probs, references=labels),
      **acc_evaluate.compute(predictions=pred_labels, references=labels),
    }
\end{lstlisting}

必要なものを一通り定義したので学習を実行します。transformersのTrainerを使っています。

\begin{lstlisting}
from transformers import (
    TrainingArguments,
    Trainer,
)

training_args = TrainingArguments(
    output_dir=f"../results/",
    num_train_epochs=10,
    learning_rate=1e-4,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    weight_decay=1.0,
    evaluation_strategy="epoch",
    logging_strategy="epoch",
    remove_unused_columns=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=ds["train"],
    eval_dataset=ds["valid"],
    tokenizer=tokenizer,
    data_collator=TokenizeCollator(tokenizer),
    compute_metrics=compute_metrics,
)

trainer.train()
\end{lstlisting}

学習を始めてすぐに、案の定GPUのメモリがあふれてエラーになってしまいました。

\begin{figure}[h]
  \centering
  \includegraphics[width=100mm]{../C105Fig/gray/ft_oom.png}
  %\caption{}
  %\label{fig:GoogleColab}
 \end{figure} 

transformersは非常に便利なライブラリで、学習の実装はかなりシンプルにできましたが、計算リソースだけはどうしようもありません。それを解決するため、次章からPEFT手法を一つ一つ見ていきます。

\newpage

\section{低ランク近似により学習するパラメータを減らす}

\subsection{LoRA: 低ランクの差分テンソルを学習}

PEFTの代表格がLow-Rank Adaptation (\textsf{LoRA})です\footnote{E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021.}。LoRAというと画像生成AIで良くも悪くも取りざたされがちですが、画像に限らずニューラルネットワークのfine-tuning一般に応用可能な技術です。というのもLoRAはテンソルパラメータのfine-tuningを省メモリで行う技術なのですが、ニューラルネットワークは基本的にテンソル演算の繰り返しなのでニューラルネットワークならLoRA適用可能と言って差し支えないためです。

もう少し詳しく書きます。通常のfine-tuningでは事前学習済みモデルのテンソルのパラメータ自体を更新していきます。それに対し、LoRAでは元のモデルのパラメータは学習中も固定（凍結）したうえで、そこからの差分となるテンソルを別途用意し、そちらだけを学習します。これだけだとテンソルが増える分無駄にGPUメモリを消費するだけなのですが、追加の工夫として低ランク近似を行います。例えば元のモデルが$1024 \times 1024$のところ、$1024 \times 8$のテンソルと$8 \times 1024$のテンソルの積に近似するといった具合です。この場合パラメータ数は$1024^2 = 1048576$個から$1024 \times 8 \times 2 = 16384$と1/100まで小さくすることができます。学習時に勾配として必要とするメモリ量は更新対象のパラメータの数に比例するので、1.8Bモデルなら増えるのはもともと7.2GBのところ、72MBまで縮められることになります。モーメントに必要なメモリも同様です。

\newpage

\begin{figure}[h]
  \centering
  \includegraphics[width=80mm]{../C105Fig/gray/lora_image.png}
  %\caption{}
  %\label{fig:GoogleColab}
 \end{figure} 

PEFTの他手法と比べたLoRAの利点としては、学習後は差分テンソルを元のテンソルと足し合わせて一つのモデルにマージすることで、推論時の計算量を元のモデルと同じにすることができるという点です。後述する他の手法ではモデルの中に追加の計算処理を入れて、その処理におけるパラメータを学習するパターンが多いです。そのケースでは推論時にも追加の計算処理を実行する必要があるため、どうしても元のモデルより計算量は増えてしまいます。それを回避できる点でLoRAは魅力的です。またLoRAの低ランク近似は、元のモデルの知識がfine-tuningによって失われるのを防止する良い正則化になっているという議論もあります\footnote{D. Biderman, J. Portes, J. J. G. Ortiz, M. Paul, P. Greengard, C. Jennings, D. King, S. Havens, V. Chiley, J. Frankle, C. Blakeney, and J. P. Cunningham, “LoRA Learns Less and Forgets Less,” arXiv preprint. arXiv:2405.09673 [cs.LG] (2024).}。

説明はこれくらいにして、実際に実行してみましょう！　PEFT手法はhuggingfaceのpeftというライブラリにまとめられていて、LoRAを含む様々な手法を統一された書き方で簡単に実装することができます。

\begin{lstlisting}
from peft import LoraConfig, TaskType
from peft.peft_model import PeftModelForSequenceClassification

# PeftModelForSequenceClassificationにバグがあるため
# 関数の差し替えをしている
class PatchedPeftModelForSequenceClassification(PeftModelForSequenceClassification):
    def add_adapter(self, adapter_name, peft_config, low_cpu_mem_usage: bool = False):
        super().add_adapter(adapter_name, peft_config)

peft_config = LoraConfig(
  task_type=TaskType.SEQ_CLS,
  r=1,
)

peft_model = PatchedPeftModelForSequenceClassification(model, peft_config)
\end{lstlisting}

たったこれだけの実装で元のモデルの重みを固定し、LoRAの低ランク差分テンソルを接続することができます。実際にどれくらい学習するパラメータが減ったか見てみましょう。

\begin{lstlisting}
peft_model.print_trainable_parameters()
# 出力: trainable params: 200,704 || all params: 1,663,870,976 || trainable%: 0.0121
\end{lstlisting}

全パラメータが16億に対して、学習するパラメータは20万と、実に0.01％程度まで減らせています。今回の設定では低ランク近似のランクを1にまで圧縮しているので、学習対象のパラメータも4桁も下げることができているのです。これにより、学習中に必要な勾配などのために保持しておくメモリ容量は誤差程度となるため、最低限のGPUメモリ消費で学習を動かせるようになります。

それでは実際に学習させてみましょう。学習スクリプトは2章3節のスクリプトにおいてTrainerに渡すmodel変数をpeft\_modelに置き換えるだけです。

\begin{figure}[h]
  \centering
  \includegraphics[width=60mm]{../C105Fig/gray/lora_train.png}
  %\caption{}
  %\label{fig:GoogleColab}
 \end{figure} 


まず、メモリのエラーを起こさずにきちんと学習が進行しました。これだけでもPEFT手法を使う意義があります。次に学習に伴うvalidデータへの精度を見ると1epochからにはAUC 0.98を超え、Accuracyは0.944に収束しています。またtestデータへのAccuracyは0.963でこちらも良好です。

タスクの準備の節で触れた通り、今回使うデータセットはPEFT手法ごとの性能を比較するには小さすぎるため、精度の値からLoRAの精度面における優位性について論じることはできません。しかし少なくとも意味のある学習ができていることは言ってよいでしょう。

\subsection{AdaLoRA: LoRAのランクをいい感じに決める}

さて、LoRAで効率よくLLMのfine-tuningができることが確認できたのは良いのですが、賢い人は次のように考えました。「LLMの全てのテンソルで同じランクの近似を行っているが、テンソルごとにランクを変えた方が良いのではないか」と。

LLMはTransformerと呼ばれるSelf Attentionと全結合を組み合わせたブロックを何層も重ねています。その層は深さに応じて役割が異なると言われており、それは浅いブロックでは単語同士の関係や短いフレーズなどの特徴を抽出し、深いほど広い文脈や抽象的な概念の情報を処理する、といった具合です。実際にこれを問題提起した\textsf{AdaLoRA}という手法の提案論文\footnote{Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and  T. Zhao, “Adaptive budget allocation for parameter-efficient fine-tuning,” arXiv preprint arXiv:2303.10512, 2023. }では、LLMの層別にLoRAを適用すると層が深いテンソルへのLoRAモデルの方が精度が上がることを確認しています。


\begin{figure}[h]
  \centering
  \includegraphics[width=100mm]{../C105Fig/gray/adalora_layers.png}
  %\caption{}
  %\label{fig:GoogleColab}
 \end{figure} 



であればブロックごとに低ランクテンソルのランクを調整しよう！　……となるのですが、たくさんある層個別にランクを指定すると組み合わせ爆発してハイパーパラメータ探索が大変になってしまいます。そこでAdaLoRAでは、学習中に層ごとの重要度を計測し、その重要度に応じてランクを割り当てるというテクニックが開発・導入されました。テクニックの詳細は原著をご参照いただければと思いますが、ざっくり説明すると、

\begin{enumerate}
  \item 低ランク近似を特異値分解の形に変換したうえで
  \item 学習が進むごとに、中央の対角テンソルの各値およびそれに紐づく左右のテンソルの行・列に対して損失関数の勾配を出し
  \item それが小さいものは精度への影響が小さい要素ということで対角テンソルの要素を0にしてしまう
\end{enumerate}

という流れにより、最初は全体的に多めにランクを割り当てたところから間引いていくことで最終的に重要度に合わせたランクに落とし込んでいきます。

と、長々書きましたが、このような細かいロジックを知らなくてもとりあえず動かすことはできます。先ほどのLoRAConfigをAdaLoRAConfigに書き換えるだけです。

\begin{lstlisting}
from peft import AdaLoraConfig, TaskType
peft_config = AdaLoraConfig(
    task_type=TaskType.SEQ_CLS,
    init_r=2,
    target_r=0.5,
)
\end{lstlisting}

最初は1テンソルあたりランク2からスタートして、最終的には1テンソルあたりランク0.5まで間引くような設定を書きました。実際には対角テンソルの非ゼロの要素が0個のテンソル、1個のテンソル、2個のテンソルが重要度に応じて決定されるという挙動を期待しています。

学習した結果は次の通りです。

\newpage

\begin{figure}[h]
  \centering
  \includegraphics[width=60mm]{../C105Fig/gray/adalora_train.png}
  %\caption{}
  %\label{fig:GoogleColab}
 \end{figure} 

LoRAに比べると少し収束までステップを要していますが、最終的にはAccuracy 0.926に落ち着きました。testデータへのAccuracyは0.944となり、良い精度が出ています。

LoRAの派生手法はAdaLoRA以外にも数多く存在します。一部はより高精度を求め、一部はより高速・省メモリになるように発展させ、huggingface/peftライブラリで利用できるものも数多くあります。このように派生が数多くあることがLoRAという手法の優秀さを物語っているといえるでしょう。

\newpage

\section{入力プロンプトを最適化する}

\subsection{Prompt tuning: 学習可能な仮想プロンプトを用いる}

LLMと言えばプロンプトエンジニアリング、すなわちLLMの出力を見ながらプロンプトを調整することで目的のタスクを解かせるような使い方が定番です。この枠組みをfine-tuningに応用したのが仮想プロンプト系の手法で、\textsf{Prompt tuning}もその一つです\footnote{B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for parameter-efficient prompt tuning,” arXiv preprint arXiv:2104.08691, 2021}。

P-Tuningの趣旨はプロンプトエンジニアリングと比較するとわかりやすいです。プロンプトエンジニアリングでは、人が様々にプロンプトを入力します。例えば以下のようなものです。

\begin{lstlisting}
次に挙げるポエムがBLEACHのものかCOMIC LOのものか分類してください: {ポエム}
\end{lstlisting}

この入力に対してLLMはまず『次に挙げるポエムがBLEACHのものかCOMIC LOのものか分類してください:』というプロンプトをまずトークン分割したうえで、Embedding層を通してトークンごとに埋め込みベクトルに変換し横につなげます。結果として、もしプロンプトが20個のトークンに分割された場合、出力は(20, ベクトル次元)のshapeを持つテンソルになります。ポエムも同様にトークン分割・埋め込みベクトルに変換し、つなげてテンソルとします。得られたテンソル同士も横につなげて一つのテンソルとしたのち、Transformerブロックでトークン間の関係を解釈していきます。

Prompt tuningでは実際のプロンプトのテンソルの代わりに、例えば(20, ベクトル次元)の学習可能なテンソルを用意し、ポエムに対応するテンソルの横につなげます。ここで20という数字はハイパーパラメータです。学習時にはこの仮想プロンプトに対する埋め込みのテンソルに対して勾配逆伝播を行いパラメータを調整していきます。またプロンプトエンジニアリングと異なり、出力層を置き換えることで分類結果のテキスト生成ではなく各クラスの確率を直接出力する分類モデルに変換することができ、より分類タスクに最適化することができます。


\begin{figure}[h]
  \centering
  \includegraphics[width=100mm]{../C105Fig/gray/prompttuning_image.png}
  % \caption{}
  %\label{fig:GoogleColab}
 \end{figure} 

 
 \rightline{\footnote{図の出典: https://research.google/blog/
 
 guiding-frozen-language-models-with-learned-soft-prompts/}}

通常のプロンプトエンジニアリングがトークンを離散的に選びながら調整する一方、Prompt tuningでは連続的に値をとれるパラメータを勾配法で調整することから、前者をハードプロンプト・後者をソフトプロンプトと呼ぶことがあります。

Prompt tuningの趣旨を説明したところで実際に動かしてみましょう。前章の低ランク近似とは全く異なる手法ですが、huggingface/peftライブラリでは全く同じ書き方でOKです。

\begin{lstlisting}
from peft import PromptTuningConfig, TaskType

peft_config = PromptTuningConfig(
    task_type=TaskType.SEQ_CLS,
    num_virtual_tokens=10,
)
\end{lstlisting}

この設定で学習した結果が以下の通りです。


\begin{figure}[h]
  \centering
  \includegraphics[width=60mm]{../C105Fig/gray/prompttuning_train.png}
  % \caption{}
  %\label{fig:GoogleColab}
 \end{figure} 


前章のLoRA・AdaLoRAとは異なりTraining Lossからして少し不安定です。一応validデータのAccuracyはそこそこ出ているものの、testデータへのAccuracyは0.815と比較的小さめに出てしまいました。

LoRAと異なりPrompt tuningは入力という勾配逆伝播の終点部分のみを学習する\footnote{一応クラスごとのlogitsを計算する全結合層も学習はしますが
}ため、勾配を伝える間にノイズが乗りやすく学習が難しいのかもしれません。とはいえデータ数が少ないですから、手法の優劣についてここで論じるのは早計であることは改めて書いておきます。

\subsection{P-Tuning: ニューラルネットワークを通して仮想プロンプト埋め込みを作る}

ソフトプロンプトを勾配法で学習するのが素朴には難しいということは知られており、埋め込みのパラメータを直接学習するのではなく学習可能なニューラルネットワークを通して埋め込みを作る\textsf{P-Tuning}という手法も提唱されています\footnote{X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang, “GPT understands, Too,” arXiv preprint arXiv:2103.10385, 2021.}。

$$
h_i = \mathrm{MLP} \left( [\overrightarrow{h}_i : \overleftarrow{h}_i] \right) \\
= \mathrm{MLP} \left( [\mathrm{LSTM}(h_{0:i}) : \mathrm{LSTM}(h_{i:m})] \right)
$$

トークン埋め込みにあたるテンソルをLSTMとMLPに通しています。この出力をポエムに対する埋め込みと繋げてTransformerブロックに入力します。Prompt tuningとの違いはLSTM・MLPに通すかです。

これにより学習結果にどのような影響があるのでしょうか？　設定は以下の通りです。

\begin{lstlisting}
from peft import PromptEncoderConfig, TaskType
  
peft_config = PromptEncoderConfig(
    task_type=TaskType.SEQ_CLS,
    num_virtual_tokens=10,
)
\end{lstlisting}

そして学習した結果が次の通りです。

\newpage

\begin{figure}[h]
  \centering
  \includegraphics[width=60mm]{../C105Fig/gray/ptuning_train.png}
  % \caption{}
  %\label{fig:GoogleColab}
 \end{figure} 

valid AUCなどは少しPrompt tuningよりも良くなっています。testデータへのAccuracyは0.852とこちらも少し改善しました。が、データ数を踏まえると誤差の範囲とも言えるかもしれません。

なおあたかもP-tuningをPrompt tuningの発展手法のように書いていますが、実際にはP-Tuningの提案論文はPrompt tuningよりも1カ月早く世に出ています。明確な優劣もついておらずタスク・データセットごとに相性があるという報告もあり\footnote{X. Liu, K. Ji, Y. Fu, W. L. Tam, Z. Du, Z. Yang, and J. Tang, “P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks,” arXiv preprint arXiv:2110.07602, 2021.}、どちらがより良い手法かについては実験的に探索するのが良いでしょう。

\newpage

\section{内部に追加のアダプターを導入する}

\subsection{$(IA)^3$: 各層の出力を補正するベクトル}

低ランクの差分テンソル・仮想プロンプトに続く第3のPEFT手法が内部に使われるアダプターを使うという手法です。その中でも$(IA)^3$はLoRAに比べてよりfine-tuningするパラメータが少ない手法として知られています\footnote{H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel, “Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning,” Advances in Neural Information Processing Systems, vol. 35, pp. 1950–1965, 2022.}。

$(IA)^3$ではTransformerブロックにおけるテンソルの一部（一般的にはSelf-AttentionのKey, ValueとFFNの1層目）の出力に対して学習可能なベクトルを定義し、各トークンに対応するベクトルに対して要素ごとに積を取ることでその出力を補正します。


\begin{figure}[h]
  \centering
  \includegraphics[width=80mm]{../C105Fig/gray/ia3_image.png}
  % \caption{}
  %\label{fig:GoogleColab}
 \end{figure} 


$(IA)^3$の利点としてはLoRAよりもさらに学習可能なパラメータ数を減らせることです。LoRAの場合はざっくり（トークンごとの埋め込み次元 $\times$ 低ランク近似のランク数 $\times$ 2 $\times$ 近似するテンソルの数 $\times$ Transformerブロックの数）分のパラメータを学習することになりますが\footnote{厳密にはFFNの1層目の埋め込み次元のみ異なる点を考慮する必要があります}、$(IA)^3$だと（トークンごとの埋め込み次元 $\times$ 学習可能なベクトルで補正するテンソルの数 $\times$ Transformerブロックの数）分のパラメータを学習するだけで済みます。ただLoRAでもかなり学習するパラメータ数を減らせることからGPUメモリの節約効果は絶対値的には大差なく、学習速度もほとんど変わらないことが多いです。

なお$(IA)^3$はLoRAと同様に学習後にはベクトルと各テンソルをかけ合わせることで、推論は元のモデルと同じ計算量で済みます。LoRAのランクのようなハイパーパラメータもないため、$(IA)^3$で精度が出る場合にはこちらを選ぶのも良いでしょう。

それでは動かしてみましょう。IA3はハイパーパラメータもほとんどなく設定もかなりシンプルです。

\begin{lstlisting}
from peft import IA3Config, TaskType
 
peft_config = IA3Config(task_type=TaskType.SEQ_CLS)
\end{lstlisting}


\newpage

\begin{figure}[h]
  \centering
  \includegraphics[width=60mm]{../C105Fig/gray/ia3_train.png}
  % \caption{}
  %\label{fig:GoogleColab}
 \end{figure} 

 
学習は比較的速く進み、validデータへの精度も安定しています。testデータのAccuracyは0.926とこちらも良好でした。

\subsection{LLaMa Adapter: }

Adapterを入れる手法としてもう一つ、LLaMa-Adapterを紹介しておきましょう。\footnote{R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao, “Llama-adapter: Efficient fine-tuning of language models with zero-init attention,” arXiv preprint arXiv:2303.16199, 2023.}

LLaMa-AdapterはTransformersブロックのSelf-Attentionの前方に仮想的な埋め込みにあたるテンソルから計算したAttentionのKeyとValueをつなげる手法です。学習可能なテンソルを前方につなげるという意味では仮想プロンプトを学習する手法に近いですが、Transformerブロックのうち出力層に近い数ブロックを対象にする点が違いとなります。主なハイパーパラメータとしては仮想的なプロンプトのトークン数と出力から何ブロックまでさかのぼって適用するかの2つがあります。


\begin{figure}[h]
  \centering
  \includegraphics[width=70mm]{../C105Fig/gray/llama_adapter_image.png}
  % \caption{}
  %\label{fig:GoogleColab}
 \end{figure} 


LLaMa-Adapterの特徴としては、仮想的なプロンプトの代わりに画像などを埋め込んで渡しても良いという点です。これにより、「画像に写っている標識はどういう意味ですか？」などのマルチモーダルなタスクに答えることができます。学習時には画像をテキストと同じ意味空間に落とし込むような学習をすることで、LLMのパラメータは固定したまま画像入力を受け取ることができるようになります\footnote{察しのいい人は気づいたかもしれませんが、同じ理屈でPrompt TuningやP-Tuningもマルチモーダル化することは可能で、そういった研究もあります。}。



\begin{figure}[h]
  \centering
  \includegraphics[width=100mm]{../C105Fig/gray/llama_adapter_image2.png}
  % \caption{}
  %\label{fig:GoogleColab}
 \end{figure} 


LLaMa-Adapterはその名前のとおりLLaMaと呼ばれるMeta社が公開するLLMのネットワーク構造に依存した手法で、他のネットワーク構造のLLMに適用するのは難しい場合があります。しかし本書で利用するllm-jp/llm-jp-3-1.8bモデルはLLaMaと同じ構造を採用しており、LLaMa-Adapterも利用可能です。さっそく試してみましょう。

\begin{lstlisting}
from peft import AdaptionPromptConfig, TaskType

peft_config = AdaptionPromptConfig(
    task_type=TaskType.SEQ_CLS,
    adapter_layers=8,
    adapter_len=10,
)
\end{lstlisting}



\begin{figure}[h]
  \centering
  \includegraphics[width=60mm]{../C105Fig/gray/llama_adapter_train.png}
  % \caption{}
  %\label{fig:GoogleColab}
 \end{figure} 

学習は非常に安定して進みました。仮想プロンプト系の手法と異なり出力層に近い部分の勾配のみが考慮されることが安定に寄与している……かもしれません。testデータへのAccuracyは0.963でこちらも良好でした。

なおLLaMa-Adapterについては$(IA)^3$などとは異なりレイヤーをマージして推論時には元のモデルと同じ推論時間というわけにはいきません。ただし計算量が増えるのはAttentionの部分だけですので、仮想トークン数が同じであっても仮想プロンプト系の手法よりは計算量は抑えられると考えられます。

\newpage

\section{終わりに}

以上が代表的なPEFT手法になります。いずれも興味深い工夫で数少ないパラメータからモデルの性能を引き出していましたね。

今や様々な企業・機関がLLMを公開しており、その性能も日進月歩です。せっかく自由にパラメータをいじれる公開LLMをプロンプトエンジニアリングだけで扱うのはもったいないことで、目的に合わせたラベル付きデータセットでfine-tuningすることでその可能性を引き出すことができます。学習するハードウェアの確保が難しいとなった場合には、ぜひ本書の内容を思い出してみてください。

誰でもプロンプトでAIを活用できるAI民主化の時代になっても、精度を突き詰めるシーンで機械学習エンジニアの舞台がなくなることはありません。AIに使われず、AIを使いこなす気概で次の時代を切り拓いていきましょう。

\newpage

\thispagestyle{empty} 

\textcolor{white}{.}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\vspace{\baselineskip}
\begin{screen}

実践PEFT ～ご家庭のGPUでLLM fine-tuning～

\begin{tabbing}
  0000000 \= 000 \= \kill
  著者 \> お椀の底の玉 \\
  発行日 \> 2024/12/30 (初版) \\
  発行元 \> ゆるふわ数理研究所　\\
  X ID \> @yurufuwasuuri \\
  印刷所 \> ちょ古っ都製本工房
\end{tabbing}

\end{screen}

\vspace{\baselineskip}
% 本書で使用したスプレッドシートは

% % \url{https://docs.google.com/spreadsheets/d/1-R35DSPx9mZpWCph}

% % \url{WSuliG8pkzbRLOUb9ySijESiDlc/edit?usp=sharing}

% で閲覧が可能です。


\end{document}
\